---
title: "Replace it"
author: " Carlos van der Ley, Christine Kwon, Connor, Leonardo Gonzalez"
output: 
  html_document:
    toc: true
    toc_float: true
    code_folding: hide
bibliography: references.bib
link-citations: true
---

## Title (replace it)



## Questions

**1. **

**2. **

**3. **

**4. **

## Data

### Data Introduction

The original data in this analysis is from Reddit and comprises 107 different datasets, including posts from 15 mental health support groups across 28 subreddits from 2018 to 2020. It was originally used, from January to April 2020, to analyze how COVID-19 affected mental health support groups[@low2020natural]. The dataset was downloaded from Open Science Framework[^1] (OSF), a free and open-source project management tool for researchers. 

**Reddit Mental Health Dataset**  
Contains posts and text features for the following timeframes from 28 mental health and non-mental health subreddits:
15 specific mental health support groups (r/EDAnonymous, r/addiction, r/alcoholism, r/adhd, r/anxiety, r/autism, r/bipolarreddit, r/bpd, r/depression, r/healthanxiety, r/lonely, r/ptsd, r/schizophrenia, r/socialanxiety, and r/suicidewatch)
2 broad mental health subreddits (r/mentalhealth, r/COVID19_support)
11 non-mental health subreddits (r/conspiracy, r/divorce, r/fitness, r/guns, r/jokes, r/legaladvice, r/meditation, r/parenting, r/personalfinance, r/relationships, r/teaching).
License
This dataset is made available under the Public Domain Dedication and License v1.0 whose full text can be found at [http://www.opendatacommons.org/licenses/pddl/1.0/](http://www.opendatacommons.org/licenses/pddl/1.0/)

[^1]: [Open Science Framework](https://osf.io/7peyq/l)

### Load packages

```{r load-packages, message=FALSE}
library(tidyverse)
library(tidymodels)
library(forcats)
library(olsrr)
library(skimr)
library(srvyr)
library(osfr) #from osf website
```

### Data Import
```{r data-import}

# IF DATA EXISTS IT WILL SKIP THIS STEP
# it takes about 5 min to download with 1GB internet connection
if(!file.exists("data/mental_health.rda")) {
  
  cidr <- getwd()
  mkfldr <- "data/"
  dir.create(file.path(cidr, mkfldr), recursive = TRUE)
  osf_retrieve_file("https://osf.io/7peyq/data/input/5efd0743af1156008d3b5532") |> 
    osf_download("data/")
  
}

```

### Data Wrangling

The original data after importing comprises 107 different datasets, with 350 variables each, in a 2.6GB compress file, but only 56 are useful for our analysis. These 56 datasets are related to the following mental health groups with four datasets each: anxiety, socialanxiety, healthanxiety,  autism, adhd, ptsd, lonely, depression, suicidewatch, bipolarreddit, bpd, addiction, alcoholism, and schizophrenia.

Steps taken to reshape the data that would be useful for our analysis:

#### Step 1

Get related mental datasets: select only datasets related to mental health groups mentioned previously. The code below will check if data already exists, if so, it will skip this step.

```{r data-wrangling, message = FALSE}

# IF DATA EXISTS IT WILL SKIP THIS STEP
if(!file.exists("data/mental_health.rda")) {
  
  # GET RELATED MENTAL DATASETS
  mental_files = "anxiety |socialanxiety | healthanxiety | autism | adhd | ptsd | lonely | depression | suicidewatch | bipolarreddit | bpd | addiction | alcoholism | schizophrenia"
  
  data <- list.files(recursive = TRUE,
                     path = "data/reddit_mental_health_dataset",
                     pattern = mental_files,
                     full.names = TRUE) |>
    purrr::map(~read_csv(.))
  
}
```

#### Step 2

Combine datasets and drop duplication: after dataset selection, we combine them all, since they have the same structure. Additionally, because two datasets overlap, **pre**: Dec 2018 to Dec 2019, and **2019**: Jan 1 to April 20, 2019, we filter the data to drop duplicate users.

```{r}

# IF DATA EXISTS IT WILL SKIP THIS STEP
if(!file.exists("data/mental_health.rda")) {
  
  # COMBINE DATASETS
  red_mh <- data |>
    map_df(bind_rows)
  
  # DROP DUPLICATE USERS
  red_mh <- unique(red_mh)
  
}
```

#### Step 3

Select important features: select only the most useful features among 350 originally.

```{r}

# IF DATA EXISTS IT WILL SKIP THIS STEP
if(!file.exists("data/mental_health.rda")) {
  
  # SELECT IMPORTANT FEATURES FROM DATASET
  features <- c("subreddit", "author",	"date", "post", "n_words", "sent_neg", "sent_neu", "sent_pos", "isolation_total", "economic_stress_total", "domestic_stress_total", "liwc_anger", "liwc_anxiety", "liwc_negative_emotion", "liwc_positive_emotion", "liwc_sadness")
  
  red_mh <- red_mh |>
    select(all_of(features))
  
}
```

#### Step 4

Split date in year, month, and day: create three new variables for date: year, month, and day, and drop date.

```{r}

# IF DATA EXISTS IT WILL SKIP THIS STEP
if(!file.exists("data/mental_health.rda")) {
  
  # SPLIT DATE IN YEAR MONTH AND DAY
  red_mh_mod <- red_mh |>
    separate(date, c("year", "month", "day"), remove = TRUE)
  
}
```

#### Step 5

Pivot sentence sentiment: group variables **sent_neg**, **sent_neu**, and **sent_pos**, that represent a sentiment of a post, into a new variable **post_sentiment**, and change the name of each category by dropping the preffix **sent_**.

Pivot liwc variables: group variables **liwc_anger**, **liwc_anxiety**, **liwc_negative_emotion**, **liwc_positive_emotion**, and **liwc_sadness**, that measure people's sentiment through people's text, into a new variable **liwc**, and change the name of each category by dropping the preffix **liwc_**.

```{r}

# IF DATA EXISTS IT WILL SKIP THIS STEP
if(!file.exists("data/mental_health.rda")) {
  
  # PIVOT SENTENCE SENTIMENT
  red_mh_mod <- red_mh_mod |> 
    pivot_longer(sent_neg:sent_pos, names_to = "post_sent", values_to = "prob_sent", names_pattern = "sent_(.*)")
  
  # PIVOT LIWC
  red_mh_mod <- red_mh_mod |> pivot_longer(liwc_anger:liwc_sadness, names_to = "people_sent", values_to = "people_sent_val", names_pattern = "liwc_(.*)")
  
}
```

#### Step 6

Save dataset and delete raw data: save the non-wrangled and wrangled datasets into the data folder, and delete 51 unnecessary and unused datasets from 107 total, releasing  about 2.5GB on disk.

```{r}

# IF DATA EXISTS IT WILL SKIP THIS STEP
if(!file.exists("data/mental_health.rda")) {
  
  # SAVE THE FINAL DATASET
  save(red_mh, file="data/mental_health.rda")
  save(red_mh_mod, file="data/mental_health_wrangled.rda")
  
  # DELETE RAW DATA: 107 FILES, +/- 2.6GB
  unlink("data/reddit_mental_health_dataset", recursive = TRUE)
}

```

#### Load the data

```{r}
# LOAD THE DATASET
load("data/mental_health.rda")
load("data/mental_health_wrangled.rda")
```

#### A (may be removed later)

#### B (may be removed later)

## Analysis



### Exploratory Data Analysis

```{r}
# skim(red_mh)
# skim(red_mh_mod)
```

## Data Analysis

### Question 1:

```r{}

```
-ANSWER QUESTION 1

### Question 2:

```r{}

```
-ANSWER QUESTION 2

### Question 3:

```r{}

```
-ANSWER QUESTION 3

### Question 4:

```r{}

```

-ANSWER QUESTION 4

## Modeling

-ADD MODELS HERE

### Results

-ADD RESULTS (WE CAN SKIP IF WERE ALREADY ANSWERED)

## Conclusion

-ADD CONCLUSION (SUMMARIZE THE QUESTIONS AND ANSWERS)
